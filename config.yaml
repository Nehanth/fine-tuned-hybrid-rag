model:
  encoder: "sentence-transformers/all-MiniLM-L6-v2"  # Base model
  device: "auto"  # auto, cpu, cuda

dataset:
  name: "leminda-ai/s2orc_small"
  processed_path: "data/processed_docs.jsonl"

# Neural fusion model settings (replaces fixed scoring weights)
neural_fusion:
  learning_rate: 0.001 # Training learning rate
  epochs: 100          # Training epochs
  batch_size: 64       # Training batch size

# Retrieval parameters
retrieval:
  top_k: 10
  candidate_pool_size: 1000

# Embedding generation settings
embeddings:
  dense_path: "embeddings/dense.npy"
  dense_finetuned_path: "embeddings/dense_finetuned.npy"
  tfidf_vectorizer_path: "embeddings/tfidf_vectorizer.pkl"
  batch_size: 256  # For embedding generation
  normalize_embeddings: true

# Fine-tuning configuration
finetune:
  # Training data
  max_pairs: 50000
  
  # Model training
  output_path: "finetune/model/"
  epochs: 1
  batch_size: 32
  warmup_steps: 100

  # Multi-GPU settings
  use_multi_gpu: false

