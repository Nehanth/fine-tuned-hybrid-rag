import json
import numpy as np
import torch
import yaml
import os
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib
from tqdm import tqdm

def load_config():
    """Load configuration from config.yaml"""
    with open("config.yaml", "r") as f:
        return yaml.safe_load(f)

def load_documents(file_path):
    """Load documents from JSONL file and return list of texts and IDs."""
    documents = []
    doc_ids = []
    
    print(f"Loading documents from {file_path}...")
    with open(file_path, "r") as f:
        for line in tqdm(f, desc="Loading documents"):
            doc = json.loads(line.strip())
            documents.append(doc["text"])
            doc_ids.append(doc["id"])
    
    print(f"Loaded {len(documents)} documents")
    return documents, doc_ids

#Dense Embeddings - Generated by embedding models like minilm
#Dense = similarity search
def generate_dense_embeddings(documents, config):
    """Generate dense embeddings using sentence transformers with GPU acceleration."""
    model_name = config["model"]["encoder"]
    device = config["model"]["device"]
    batch_size = config["embeddings"]["batch_size"]
    normalize = config["embeddings"]["normalize_embeddings"]
    
    # Set device
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"Using device: {device}")
    if torch.cuda.is_available():
        print(f"Available GPUs: {torch.cuda.device_count()}")
        print(f"GPU 0: {torch.cuda.get_device_name(0)}")
    
    print(f"Loading sentence transformer model: {model_name}")
    model = SentenceTransformer(model_name, device=device)
    
    print("Generating dense embeddings...")
    # Use config-based settings
    embeddings = model.encode(
        documents, 
        show_progress_bar=True,
        batch_size=batch_size if device == "cuda" else batch_size // 2,
        convert_to_numpy=True,
        normalize_embeddings=normalize
    )
    
    print(f"Generated embeddings shape: {embeddings.shape}")
    return embeddings

#Sparse Embeddings - Generated by TF-IDF vectorizer
#Sparse = keyword matching
def generate_sparse_embeddings(documents, max_features=10000):
    """Generate sparse TF-IDF embeddings."""
    print(f"Generating TF-IDF embeddings with max_features={max_features}")
    
    vectorizer = TfidfVectorizer(
        max_features=max_features,
        stop_words='english',
        lowercase=True,
        ngram_range=(1, 2)  # Include bigrams for better semantic matching
    )
    
    print("Fitting TF-IDF vectorizer...")
    tfidf_matrix = vectorizer.fit_transform(tqdm(documents, desc="Vectorizing documents"))
    
    print(f"Generated TF-IDF matrix shape: {tfidf_matrix.shape}")
    return vectorizer, tfidf_matrix

def save_embeddings(dense_embeddings, tfidf_vectorizer, doc_ids, config):
    """Save embeddings and vectorizer to disk."""
    print("Saving embeddings...")
    
    # Create embeddings directory if it doesn't exist
    os.makedirs("embeddings", exist_ok=True)
    
    # Save dense embeddings
    dense_path = config["embeddings"]["dense_path"]
    np.save(dense_path, dense_embeddings)
    print(f"Saved dense embeddings to: {dense_path}")
    
    # Save TF-IDF vectorizer
    tfidf_path = config["embeddings"]["tfidf_vectorizer_path"]
    joblib.dump(tfidf_vectorizer, tfidf_path)
    print(f"Saved TF-IDF vectorizer to: {tfidf_path}")
    
    print("Embeddings generation complete!")

def main():
    """Main function to generate and save embeddings."""
    print("=" * 60)
    print("Embedding Generation")
    print("=" * 60)
    
    # Load configuration
    config = load_config()
    
    # Load documents from config path
    doc_path = config["dataset"]["processed_path"]
    print(f"Using dataset: {doc_path}")
    documents, doc_ids = load_documents(doc_path)
    
    # Generate dense embeddings with config settings
    dense_embeddings = generate_dense_embeddings(documents, config)
    
    # Generate sparse embeddings
    tfidf_vectorizer, tfidf_matrix = generate_sparse_embeddings(documents)
    
    # Save embeddings using config paths
    save_embeddings(dense_embeddings, tfidf_vectorizer, doc_ids, config)
    
    print("\n" + "=" * 60)
    print("Embeddings ready for retrieval system!")
    print("Next steps:")
    print("1. Test retrieval: python test_retrieval.py")
    print("2. Run evaluation: python eval/beir_evaluation.py")
    print("=" * 60)

if __name__ == "__main__":
    main() 